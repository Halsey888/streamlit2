import streamlit as st
import google.generativeai as genai
import os

# é é¢é…ç½®
st.set_page_config(page_title="Gemini å°èªªå‰µä½œè»å¸«", layout="wide")
st.title("ğŸ–‹ï¸ Gemini å°èªªå‰µä½œè»å¸«ï¼šå†·é…·ç·¨è¼¯æ¨¡å¼")

# --- å´é‚Šæ¬„ï¼šè¨­å®š ---
with st.sidebar:
    st.header("âš™ï¸ å¯«ä½œè¨­å®š")
    api_key = st.text_input("è¼¸å…¥ Gemini API Key", type="password")
    
    st.divider()
    novel_path = st.text_input("ã€æˆ‘çš„ä½œå“ã€‘è·¯å¾‘ (ä½ çš„è‰ç¨¿æª”)")
    # æ–°å¢åƒè€ƒè³‡æ–™è·¯å¾‘
    ref_path = st.text_input("ã€é¢¨æ ¼åƒè€ƒã€‘è·¯å¾‘ (å­˜æ”¾ä½ å–œæ­¡çš„éŸ“åœ‹ç¶²æ–‡ç¿»è­¯)")
    
    model_name = st.selectbox("é¸æ“‡æ¨¡å‹", ["gemini-2.5-pro", "gemini-2.5-flash"])
    clear_chat = st.button("æ¸…ç©ºå°è©±ç´€éŒ„")

# --- è®€å–å·¥å…· (é€šç”¨) ---
def get_folder_content(path, label="å…§å®¹"):
    content = ""
    if not path or not os.path.exists(path):
        return ""
    valid_extensions = ('.txt', '.md')
    for root, _, files in os.walk(path):
        for file in files:
            if file.endswith(valid_extensions):
                try:
                    with open(os.path.join(root, file), 'r', encoding='utf-8') as f:
                        content += f"\n\n--- [{label}] æª”æ¡ˆ: {file} ---\n{f.read()}\n"
                except: pass
    return content

# --- åˆå§‹åŒ– AI ---
if api_key:
    genai.configure(api_key=api_key)
    
    # é‡å°éŸ“åœ‹ç¶²æ–‡é¢¨æ ¼å„ªåŒ–çš„æŒ‡ä»¤
    system_prompt = (
        "ä½ æ˜¯ä¸€ä½ç²¾é€šéŸ“åœ‹ç¶²æ–‡ï¼ˆNaver Series/KakaoPage é¢¨æ ¼ï¼‰çš„è³‡æ·±ç·¨è¼¯ã€‚"
        "ä½ æœƒåˆ†æä½¿ç”¨è€…æä¾›çš„ã€é¢¨æ ¼åƒè€ƒè³‡æ–™ã€ï¼Œå­¸ç¿’å…¶æ•˜äº‹ç¯€å¥ã€æ–‡å­—å¼µåŠ›èˆ‡è§’è‰²äº’å‹•æ–¹å¼ã€‚"
        "\n\nä½ çš„ä»»å‹™è¦å‰‡ï¼š\n"
        "1. **é¢¨æ ¼ä¸€è‡´æ€§**ï¼šç•¶æˆ‘å¯«ä½œæ™‚ï¼Œè«‹ç¢ºä¿å»ºè­°çš„æ–‡å­—ç¬¦åˆéŸ“åœ‹ç¶²æ–‡é‚£ç¨®ã€å¿«ç¯€å¥ã€ç•«é¢æ„Ÿå¼·ã€æƒ…æ„Ÿå¼·çƒˆã€çš„ç‰¹å¾µã€‚\n"
        "2. **æ‹’çµ•å¹³åº¸**ï¼šå¦‚æœæˆ‘çš„æè¿°å¤ªéæº«åï¼Œè«‹æ¨¡ä»¿åƒè€ƒè³‡æ–™ä¸­çš„å¼µåŠ›é€²è¡Œæ¯’èˆŒæ‰¹è©•ä¸¦æ”¹å¯«ã€‚\n"
        "3. **å°ˆæœ‰åè©èˆ‡èªæ°£**ï¼šæ³¨æ„åƒè€ƒè³‡æ–™ä¸­çš„ç¿»è­¯èªæ°£ï¼Œåœ¨æ”¹å¯«ç¯„ä¾‹ä¸­ä¿æŒä¸€è‡´ã€‚"
    )
    if "chat" not in st.session_state or clear_chat:
        model = genai.GenerativeModel(model_name=model_name, system_instruction=system_prompt)
        st.session_state.chat = model.start_chat(history=[])
        st.session_state.messages = []

# --- è®€å–å°èªªæ–‡æœ¬å·¥å…· ---
def get_novel_context(path):
    context = ""
    if not path or not os.path.exists(path):
        return ""
    
    # åªè®€å–å¸¸è¦‹çš„æ–‡æœ¬æª”æ¡ˆ
    valid_extensions = ('.txt', '.md', '.markdown')
    for root, dirs, files in os.walk(path):
        for file in files:
            if file.endswith(valid_extensions):
                try:
                    with open(os.path.join(root, file), 'r', encoding='utf-8') as f:
                        # æ¨™è¨»æª”åï¼Œè®“ AI çŸ¥é“é€™æ˜¯å“ªä¸€ç« æˆ–å“ªå€‹è¨­å®šæª”
                        context += f"\n\n--- æª”æ¡ˆå…§å®¹: {file} ---\n{f.read()}\n"
                except: pass
    return context

# --- ä¸»ä»‹é¢ï¼šé¡¯ç¤ºå°è©± ---
if "messages" in st.session_state:
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

# --- ä¸»é‚è¼¯ï¼šç™¼é€è¨Šæ¯ ---
if prompt := st.chat_input("è«‹æè¿°ä½ çš„å•é¡Œ..."):
    # ... (çœç•¥)
    
    # çµ„åˆä¸‰ç¨® Contextï¼šåƒè€ƒè³‡æ–™ + æˆ‘çš„è‰ç¨¿ + ç•¶å‰å•é¡Œ
    my_draft = get_folder_content(novel_path, label="æˆ‘çš„è‰ç¨¿")
    ref_style = get_folder_content(ref_path, label="éŸ“åœ‹ç¶²æ–‡é¢¨æ ¼åƒè€ƒ")
    
    full_prompt = (
        f"ã€é¢¨æ ¼åƒè€ƒåŸºæº–ã€‘(è«‹å­¸ç¿’æ­¤é¡æ–‡å­—çš„ç¯€å¥èˆ‡èªæ°£):\n{ref_style}\n\n"
        f"ã€æˆ‘çš„ç›®å‰ä½œå“å…§å®¹ã€‘:\n{my_draft}\n\n"
        f"ã€ç•¶å‰ä»»å‹™ã€‘:\n{prompt}"
    )
    # å‘¼å« Gemini ä¸¦é¡¯ç¤ºå›è¦†
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
            
        try:
            response = st.session_state.chat.send_message(full_prompt, stream=True)
            for chunk in response:
                full_response += chunk.text
                response_placeholder.markdown(full_response + "â–Œ")
            response_placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
        except Exception as e:
            st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")