import streamlit as st
import google.generativeai as genai
import os
import random

# 1. é é¢é…ç½®
st.set_page_config(page_title="Gemini éŸ“æ¼«é¢¨æ ¼åŠ©æ‰‹", layout="wide")
st.title("ğŸ–‹ï¸ Gemini å°èªªåŠ©æ‰‹ (æµ·é‡åƒè€ƒæœ€ä½³åŒ–ç‰ˆ)")

# 2. åˆå§‹åŒ– Session State
if "messages" not in st.session_state:
    st.session_state.messages = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# --- å´é‚Šæ¬„ï¼šè¨­å®š ---
with st.sidebar:
    st.header("âš™ï¸ å¯«ä½œè¨­å®š")
    api_key = st.text_input("è¼¸å…¥ Gemini API Key", type="password")
    
    st.divider()
    novel_path = st.text_input("ã€æˆ‘çš„è‰ç¨¿è·¯å¾‘ã€‘")
    ref_path = st.text_input("ã€æµ·é‡åƒè€ƒè·¯å¾‘ã€‘")
    
    # å¢åŠ å®‰å…¨é™åˆ¶è¨­å®š
    max_ref_chars = st.slider("åƒè€ƒè³‡æ–™å­—æ•¸ä¸Šé™ (å»ºè­° 20è¬å­—å…§)", 10000, 500000, 100000)
    read_mode = st.radio("åƒè€ƒè³‡æ–™è®€å–æ¨¡å¼", ["éš¨æ©ŸæŠ½æ¨£ (æ¨è–¦)", "è®€å–å‰å¹¾æª”"])
    
    model_name = st.selectbox("é¸æ“‡æ¨¡å‹", ["gemini-2.5-pro", "gemini-2.5-flash"])
    
    if st.button("ğŸš¨ æ¸…é™¤å°è©±ç´€éŒ„"):
        st.session_state.messages = []
        st.session_state.chat_history = []
        st.rerun()

# --- å„ªåŒ–å¾Œçš„è®€å–å·¥å…· ---
def get_safe_content(path, max_chars=100000, mode="éš¨æ©ŸæŠ½æ¨£ (æ¨è–¦)"):
    all_content = []
    if not path or not os.path.exists(path):
        return ""
    
    files = [f for f in os.listdir(path) if f.endswith(('.txt', '.md'))]
    
    if mode == "éš¨æ©ŸæŠ½æ¨£ (æ¨è–¦)":
        random.shuffle(files) # æ‰“äº‚é †åºï¼Œè®“ AI çœ‹åˆ°ä¸åŒç« ç¯€çš„é¢¨æ ¼
        
    current_chars = 0
    context = ""
    for file in files:
        if current_chars > max_chars:
            break
        try:
            with open(os.path.join(path, file), 'r', encoding='utf-8') as f:
                text = f.read()
                context += f"\n\n--- [åƒè€ƒæª”: {file}] ---\n{text[:5000]}\n" # æ¯å€‹æª”æ¡ˆå–ç²¾è¯éƒ¨åˆ†
                current_chars += len(text)
        except: pass
    return context

# --- åˆå§‹åŒ– AI ---
if api_key:
    genai.configure(api_key=api_key)
    
    # é‡å°éŸ“åœ‹ç¶²æ–‡é¢¨æ ¼å„ªåŒ–çš„æŒ‡ä»¤
    system_prompt = (
        "ä½ æ˜¯ä¸€ä½ç²¾é€šéŸ“åœ‹ç¶²æ–‡ï¼ˆNaver Series/KakaoPage é¢¨æ ¼ï¼‰çš„è³‡æ·±ç·¨è¼¯ã€‚"
        "ä½ æœƒåˆ†æä½¿ç”¨è€…æä¾›çš„ã€é¢¨æ ¼åƒè€ƒè³‡æ–™ã€ï¼Œå­¸ç¿’å…¶æ•˜äº‹ç¯€å¥ã€æ–‡å­—å¼µåŠ›èˆ‡è§’è‰²äº’å‹•æ–¹å¼ã€‚"
        "\n\nä½ çš„ä»»å‹™è¦å‰‡ï¼š\n"
        "1. **é¢¨æ ¼ä¸€è‡´æ€§**ï¼šç•¶æˆ‘å¯«ä½œæ™‚ï¼Œè«‹ç¢ºä¿å»ºè­°çš„æ–‡å­—ç¬¦åˆéŸ“åœ‹ç¶²æ–‡é‚£ç¨®ã€ç•«é¢æ„Ÿå¼·ã€è§’è‰²æƒ…æ„Ÿæå¯«ç´°è†©ã€é »ç¹æ›è¡Œã€çš„ç‰¹å¾µã€‚\n"
        "2. **å°ˆæ¥­åˆ†æ**ï¼šå¦‚æœæˆ‘çš„æè¿°å¤ªéæº«åï¼Œè«‹æ¨¡ä»¿åƒè€ƒè³‡æ–™ä¸­çš„å¼µåŠ›é€²è¡Œé©ç•¶æ‰¹è©•ä¸¦æ”¹å¯«ã€‚\n"
        "3. **å°ˆæœ‰åè©èˆ‡èªæ°£**ï¼šæ³¨æ„åƒè€ƒè³‡æ–™ä¸­çš„ç¿»è­¯èªæ°£ï¼Œåœ¨æ”¹å¯«ç¯„ä¾‹ä¸­ä¿æŒä¸€è‡´ã€‚"
        "4. **å…§éƒ¨æ€è€ƒ**ï¼šå›ç­”å‰å…ˆåˆ†æè§’è‰²å‹•æ©Ÿèˆ‡ç¯€å¥æ„Ÿã€‚"
    )
    # å»ºç«‹æ¨¡å‹èˆ‡å°è©±ç‰©ä»¶ï¼ˆå°‡æ­·å²ç´€éŒ„å‚³å…¥ï¼‰
    model = genai.GenerativeModel(model_name=model_name, system_instruction=system_prompt)
    # é€™è£¡ä½¿ç”¨ st.session_state.chat_history ä¾†ä¿æŒé€£è²«æ€§
    chat_session = model.start_chat(history=st.session_state.chat_history)

# --- ä¸»ä»‹é¢ï¼šé¡¯ç¤ºæ­·å²è¨Šæ¯ ---
# æ¯æ¬¡ç•«é¢é‡æ–°æ•´ç†æ™‚ï¼Œéƒ½æœƒå¾ session_state æŠ“å‡ºä¾†é‡æ–°ç•«ä¸€æ¬¡
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- ä½¿ç”¨è€…è¼¸å…¥ ---
if prompt := st.chat_input("è¼¸å…¥ä½ æƒ³è¨è«–çš„æ®µè½..."):
    if not api_key:
        st.error("è«‹å…ˆè¼¸å…¥ API Keyï¼")
    else:
        # 1. é¡¯ç¤ºä¸¦å„²å­˜ä½¿ç”¨è€…è¨Šæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # é€™è£¡ä½¿ç”¨å®‰å…¨è®€å–æ©Ÿåˆ¶ï¼Œé¿å…çˆ† Token
        ref_style = get_safe_content(ref_path, max_chars=max_ref_chars, mode=read_mode)
        my_draft = get_safe_content(novel_path, max_chars=50000) # è‰ç¨¿é™åˆ¶
        
        full_prompt = (
            f"ã€é¢¨æ ¼åƒè€ƒåŸºæº–ã€‘:\n{ref_style}\n\n"
            f"ã€æˆ‘çš„ä½œå“ä¸Šä¸‹æ–‡ã€‘:\n{my_draft}\n\n"
            f"ã€ä½¿ç”¨è€…å•é¡Œã€‘: {prompt}"
        )

        # 3. å‘¼å« Gemini ä¸¦é¡¯ç¤ºæµå¼å›è¦†
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            
            try:
                # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ chat_session.send_messageï¼Œå®ƒæœƒè‡ªå‹•è™•ç†æ­·å²ç´€éŒ„
                response = chat_session.send_message(full_prompt, stream=True)
                for chunk in response:
                    full_response += chunk.text
                    response_placeholder.markdown(full_response + "â–Œ")
                response_placeholder.markdown(full_response)
                
                # 4. é‡è¦ï¼šæ›´æ–° session_state è£¡çš„ç´€éŒ„
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                # åŒæ­¥æ›´æ–° chat_session çš„æ­·å²å› st.session_state
                st.session_state.chat_history = chat_session.history
                
            except Exception as e:
                st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
