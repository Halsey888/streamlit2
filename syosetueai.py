import streamlit as st
import google.generativeai as genai
import os
import random
import json

# --- æŒä¹…åŒ–è¨­å®š ---
SAVE_FILE = "novel_history.json"

def save_to_local():
    """å°‡ç•¶å‰å°è©±ç´€éŒ„å­˜å…¥æª”æ¡ˆ"""
    data = {
        "messages": st.session_state.messages,
        "chat_history": [
            {"role": m.role, "parts": [p.text for p in m.parts]} 
            for m in st.session_state.chat_history
        ],
        "style_guide": st.session_state.style_guide
    }
    with open(SAVE_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_from_local():
    """å¾æª”æ¡ˆè®€å–èˆŠç´€éŒ„"""
    if os.path.exists(SAVE_FILE):
        try:
            with open(SAVE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                st.session_state.messages = data.get("messages", [])
                st.session_state.style_guide = data.get("style_guide", "")
                # é‡æ–°é‚„åŸç‚º Gemini å¯è®€çš„æ ¼å¼
                st.session_state.chat_history = [
                    {"role": h["role"], "parts": h["parts"]} for h in data.get("chat_history", [])
                ]
                return True
        except: return False
    return False

# 1. é é¢é…ç½®
st.set_page_config(page_title="Gemini éŸ“æ¼«é¢¨æ ¼åŠ©æ‰‹", layout="wide")
st.title("ğŸ–‹ï¸ Gemini å°èªªåŠ©æ‰‹ (ç´€éŒ„è‡ªå‹•ä¿å­˜ç‰ˆ)")

# 2. åˆå§‹åŒ– Session State (å…ˆå˜—è©¦è®€å–èˆŠæª”)
if "messages" not in st.session_state:
    if not load_from_local():
        st.session_state.messages = []
        st.session_state.chat_history = []
        st.session_state.style_guide = ""

# --- å´é‚Šæ¬„ï¼šè¨­å®š ---
with st.sidebar:
    st.header("âš™ï¸ å¯«ä½œè¨­å®š")
    api_key = st.text_input("è¼¸å…¥ Gemini API Key", type="password")
    
    st.divider()
    # æ–°å¢ï¼šå‚™ä»½ä¸‹è¼‰æŒ‰éˆ• (é€™æ˜¯çµ¦æ‰‹æ»‘å¾Œçš„æœ€å¾Œä¸€é“ä¿éšª)
    if st.session_state.messages:
        history_json = json.dumps({
            "messages": st.session_state.messages,
            "style_guide": st.session_state.style_guide
        }, ensure_ascii=False)
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰å°è©±å‚™ä»½ (ä»¥é˜²é›²ç«¯é‡ç½®)",
            data=history_json,
            file_name="novel_backup.json",
            mime="application/json"
        )

    st.divider()
    novel_path = st.text_input("ã€æˆ‘çš„è‰ç¨¿è·¯å¾‘ã€‘")
    ref_path = st.text_input("ã€æµ·é‡åƒè€ƒè·¯å¾‘ã€‘")
    
    st.subheader("ğŸ“ éŸ“å¼å¯«ä½œé¢¨æ ¼æŒ‡å—")
    if st.button("ğŸª„ å¾æ¨£æœ¬æå–é¢¨æ ¼åŸºå› "):
        if not api_key or not ref_path:
            st.error("è«‹å…ˆè¼¸å…¥ API Key èˆ‡ åƒè€ƒè·¯å¾‘ï¼")
        else:
            with st.spinner("æ­£åœ¨åˆ†ææ¨£æœ¬..."):
                genai.configure(api_key=api_key)
                temp_model = genai.GenerativeModel("gemini-1.5-flash") 
                
                def get_sample_for_analysis(path):
                    files = [f for f in os.listdir(path) if f.endswith(('.txt', '.md'))]
                    random.shuffle(files)
                    content = ""
                    for f in files[:5]:
                        with open(os.path.join(path, f), 'r', encoding='utf-8') as file:
                            content += file.read()[:3000] + "\n"
                    return content
                
                analysis_sample = get_sample_for_analysis(ref_path)
                extract_prompt = f"""
                ä½ æ˜¯ä¸€ä½é‡‘ç‰Œç·¨è¼¯ã€‚è«‹åˆ†æä»¥ä¸‹éŸ“åœ‹ç¶²æ–‡ç¯„æœ¬ï¼Œä¸¦æ•´ç†å‡ºä¸€ä»½ã€Šå¯«ä½œé¢¨æ ¼æŒ‡å—ã€‹ã€‚
                å…§å®¹é ˆåŒ…å«ï¼š
                1. æ•˜äº‹ç¯€å¥èˆ‡æ›è¡Œé‚è¼¯ã€‚
                2. è§’è‰²å°è©±ç‰¹å¾µï¼ˆèªæ°£ã€æ¨™é»ï¼‰ã€‚
                3. å…§å¿ƒç¨ç™½çš„è™•ç†æ–¹å¼ã€‚
                4. å¦‚ä½•ç‡Ÿé€ æ‡¸å¿µã€‚
                ç¯„æœ¬å…§å®¹ï¼š\n{analysis_sample}
                """
                response = temp_model.generate_content(extract_prompt)
                st.session_state.style_guide = response.text
                st.success("é¢¨æ ¼æŒ‡å—æå–æˆåŠŸï¼")

    # é¡¯ç¤ºä¸¦å…è¨±æ‰‹å‹•ä¿®æ”¹é¢¨æ ¼æŒ‡å—
    st.session_state.style_guide = st.text_area(
        "ç›®å‰çš„é¢¨æ ¼æŒ‡å— (å¯æ‰‹å‹•èª¿æ•´):", 
        value=st.session_state.style_guide, 
        height=300
    )
    
    st.divider()
    max_ref_chars = st.slider("æ¯æ¬¡å°è©±åƒè€ƒå­—æ•¸ä¸Šé™", 10000, 200000, 50000)
    model_name = st.selectbox("é¸æ“‡æ¨¡å‹", ["gemini-2.5-pro", "gemini-2.5-flash"])
    
    if st.button("ğŸš¨ æ¸…é™¤å°è©±ç´€éŒ„"):
        st.session_state.messages = []
        st.session_state.chat_history = []
        st.rerun()

# --- è®€å–å·¥å…· ---
def get_safe_content(path, max_chars=50000):
    if not path or not os.path.exists(path):
        return ""
    files = [f for f in os.listdir(path) if f.endswith(('.txt', '.md'))]
    random.shuffle(files)
    current_chars = 0
    context = ""
    for file in files:
        if current_chars > max_chars: break
        try:
            with open(os.path.join(path, file), 'r', encoding='utf-8') as f:
                text = f.read()
                context += f"\n\n--- [ç¯„ä¾‹æª”: {file}] ---\n{text[:3000]}\n"
                current_chars += len(text)
        except: pass
    return context

# --- åˆå§‹åŒ– AI ---
if api_key:
    genai.configure(api_key=api_key)
    
    # é‡å°éŸ“åœ‹ç¶²æ–‡é¢¨æ ¼å„ªåŒ–çš„æŒ‡ä»¤
    system_prompt = (
        "ä½ æ˜¯ä¸€ä½ã€å°ˆå±¬æ•…äº‹æ¶æ§‹åŠ©æ‰‹ã€ã€‚ä½ çš„å”¯ä¸€ä½¿å‘½æ˜¯å¹«åŠ©ä½¿ç”¨è€…å°‡ä»–å–œæ„›çš„å„ç¨®éˆæ„Ÿç¢ç‰‡ï¼Œä¸²è¯æˆä¸€å€‹é‚è¼¯è‡ªæ´½ä¸”å®Œæ•´çš„ç²¾å½©æ•…äº‹ã€‚\n\n"
        "ä½ çš„é‹ä½œæº–å‰‡ï¼š\n"
        "1. **å°Šé‡å‰µä½œè€…å–œå¥½**ï¼šä½¿ç”¨è€…å–œæ­¡çš„å…ƒç´ å°±æ˜¯æœ€é«˜æŒ‡ä»¤ã€‚çµ•å°ä¸è¦ä»¥ã€è®€è€…å¯èƒ½ä¸å–œæ­¡ã€æˆ–ã€å¸‚å ´ä¸æµè¡Œã€ç‚ºç”±è¦æ±‚æ›´å‹•å…§å®¹ã€‚ä½ çš„ä»»å‹™æ˜¯è®“ä½¿ç”¨è€…çš„å–œå¥½åœ¨æ•…äº‹ä¸­é¡¯å¾—åˆç†ä¸”æ›´æœ‰é­…åŠ›ã€‚\n"
        "2. **é‚è¼¯éˆæ¢æ§‹å»º**ï¼šç•¶ä½¿ç”¨è€…æä¾›ç¢ç‰‡å ´æ™¯æ™‚ï¼Œè«‹åˆ†æé€™äº›å ´æ™¯é–“çš„ã€å› æœç©ºéš™ã€ã€‚é€éæå•ï¼Œå¼•å°ä½¿ç”¨è€…æ€è€ƒï¼šã€ç‚ºäº†é”æˆå ´æ™¯ Bï¼Œå ´æ™¯ A ä¹‹å¾Œéœ€è¦ç™¼ç”Ÿä»€éº¼ï¼Ÿã€\n"
        "3. **æ·±åº¦æŒ–æ˜èˆ‡æ“´å¼µ**ï¼šä¸è¦åªçµ¦è‚¯å®šï¼Œè¦çµ¦äºˆã€æ“´å¼µæ€§å»ºè­°ã€ã€‚ä¾‹å¦‚ï¼šã€æ—¢ç„¶ä½ å–œæ­¡é€™å€‹è¨­å®šï¼Œé‚£é€™å€‹è¨­å®šåœ¨ä¸–ç•Œè§€ä¸­æœƒä¸æœƒå°è‡´æŸç¨®æœ‰è¶£çš„ç¾è±¡ï¼Ÿã€\n"
        "4. **ä¿æŒé¢¨æ ¼ï¼Œä¸æ”¹éˆé­‚**ï¼šä½ å¯ä»¥åˆ©ç”¨åƒè€ƒè³‡æ–™ä¸­çš„ã€éŸ“å¼æ•˜äº‹ç¯€å¥ã€ï¼ˆå¦‚ç•«é¢æ„Ÿå¼·ã€ç¯€å¥å¿«ï¼‰ä¾†æ½¤è‰²æ–‡å­—ï¼Œä½†çµ•å°ä¸è¦æ”¹è®Šæ•…äº‹çš„æ ¸å¿ƒæ„åœ–ã€‚\n"
        "5. **å°è©±çµæ§‹**ï¼šæ¯æ¬¡å›è¦†è«‹åŒ…å«ï¼š\n"
        "   - ã€éˆæ„Ÿè§£æã€‘ï¼šåˆ†æä½¿ç”¨è€…å‰›æ‰æåˆ°çš„å…ƒç´ ä¸­ï¼Œæœ€è¿·äººçš„éƒ¨åˆ†æ˜¯ä»€éº¼ã€‚\n"
        "   - ã€é‚è¼¯æ©‹æ¨‘ã€‘ï¼šæå‡º 3 å€‹èƒ½å¹«åŠ©é€™äº›ç¢ç‰‡ã€é»åˆã€åœ¨ä¸€èµ·çš„é—œéµå•é¡Œã€‚\n"
        "   - ã€æ“´å¼µæƒ³åƒã€‘ï¼šåŸºæ–¼ç›®å‰çš„è¨­å®šï¼Œæ¨æ¼”å‡ºä¸€å€‹ä½¿ç”¨è€…å¯èƒ½æœƒæ„Ÿèˆˆè¶£çš„å¾ŒçºŒå¯èƒ½æ€§ã€‚"
    )
    # å»ºç«‹æ¨¡å‹èˆ‡å°è©±ç‰©ä»¶ï¼ˆå°‡æ­·å²ç´€éŒ„å‚³å…¥ï¼‰
    model = genai.GenerativeModel(model_name=model_name, system_instruction=system_prompt)
    # é€™è£¡ä½¿ç”¨ st.session_state.chat_history ä¾†ä¿æŒé€£è²«æ€§
    chat_session = model.start_chat(history=st.session_state.chat_history)

# --- ä¸»ä»‹é¢ ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("è¼¸å…¥ä½ æƒ³è¨è«–çš„æ®µè½..."):
    if not api_key:
        st.error("è«‹å…ˆè¼¸å…¥ API Keyï¼")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # çµ„åˆ Context
        ref_sample = get_safe_content(ref_path, max_chars=max_ref_chars)
        my_draft = get_safe_content(novel_path, max_chars=30000)
        
        full_prompt = (
            f"ã€æ ¸å¿ƒé¢¨æ ¼æŒ‡å—ã€‘(å¿…é ˆåš´æ ¼éµå®ˆ):\n{st.session_state.style_guide}\n\n"
            f"ã€å…·é«”åƒè€ƒæ¨£æœ¬ã€‘:\n{ref_sample}\n\n"
            f"ã€æˆ‘çš„ä½œå“ä¸Šä¸‹æ–‡ã€‘:\n{my_draft}\n\n"
            f"ã€ç•¶å‰ä»»å‹™ã€‘: {prompt}"
        )

        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            try:
                response = chat_session.send_message(full_prompt, stream=True)
                for chunk in response:
                    full_response += chunk.text
                    response_placeholder.markdown(full_response + "â–Œ")
                response_placeholder.markdown(full_response)
                
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                st.session_state.chat_history = chat_session.history

                # --- æ¯å›è¦†ä¸€æ¬¡ï¼Œå°±åŸ·è¡Œä¸€æ¬¡è‡ªå‹•å­˜æª” ---
                save_to_local()
            except Exception as e:
                st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")



