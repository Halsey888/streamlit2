import streamlit as st
import google.generativeai as genai
import os
import random

# 1. é é¢é…ç½®
st.set_page_config(page_title="Gemini éŸ“æ¼«é¢¨æ ¼åŠ©æ‰‹", layout="wide")
st.title("ğŸ–‹ï¸ Gemini å°èªªåŠ©æ‰‹ (é¢¨æ ¼æŒ‡å—å¼·åŒ–ç‰ˆ)")

# 2. åˆå§‹åŒ– Session State
if "messages" not in st.session_state:
    st.session_state.messages = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "style_guide" not in st.session_state:
    st.session_state.style_guide = "" # å­˜æ”¾æå–å‡ºçš„é¢¨æ ¼æŒ‡å—

# --- å´é‚Šæ¬„ï¼šè¨­å®š ---
with st.sidebar:
    st.header("âš™ï¸ å¯«ä½œè¨­å®š")
    api_key = st.text_input("è¼¸å…¥ Gemini API Key", type="password")
    
    st.divider()
    novel_path = st.text_input("ã€æˆ‘çš„è‰ç¨¿è·¯å¾‘ã€‘")
    ref_path = st.text_input("ã€æµ·é‡åƒè€ƒè·¯å¾‘ã€‘")
    
    # é¢¨æ ¼æŒ‡å—å€å¡Š
    st.subheader("ğŸ“ éŸ“å¼å¯«ä½œé¢¨æ ¼æŒ‡å—")
    if st.button("ğŸª„ å¾æ¨£æœ¬æå–é¢¨æ ¼åŸºå› "):
        if not api_key or not ref_path:
            st.error("è«‹å…ˆè¼¸å…¥ API Key èˆ‡ åƒè€ƒè·¯å¾‘ï¼")
        else:
            with st.spinner("æ­£åœ¨åˆ†æ 150 è¬å­—æ¨£æœ¬ä¸­çš„é¢¨æ ¼ç²¾è¯..."):
                # é€™è£¡èª¿ç”¨ AI é€²è¡Œä¸€æ¬¡æ€§åˆ†æ
                genai.configure(api_key=api_key)
                temp_model = genai.GenerativeModel("gemini-2.5-flash") # ç”¨ Flash æå–çœéŒ¢åˆå¿«
                
                # æŠ½æ¨£è®€å–åƒè€ƒè³‡æ–™
                def get_sample_for_analysis(path):
                    files = [f for f in os.listdir(path) if f.endswith(('.txt', '.md'))]
                    random.shuffle(files)
                    content = ""
                    for f in files[:5]: # æŠ½ 5 å€‹æª”
                        with open(os.path.join(path, f), 'r', encoding='utf-8') as file:
                            content += file.read()[:3000] + "\n"
                    return content
                
                analysis_sample = get_sample_for_analysis(ref_path)
                extract_prompt = f"""
                ä½ æ˜¯ä¸€ä½é‡‘ç‰Œç·¨è¼¯ã€‚è«‹åˆ†æä»¥ä¸‹éŸ“åœ‹ç¶²æ–‡ç¯„æœ¬ï¼Œä¸¦æ•´ç†å‡ºä¸€ä»½ã€Šå¯«ä½œé¢¨æ ¼æŒ‡å—ã€‹ã€‚
                å…§å®¹é ˆåŒ…å«ï¼š
                1. æ•˜äº‹ç¯€å¥èˆ‡æ›è¡Œé‚è¼¯ã€‚
                2. è§’è‰²å°è©±ç‰¹å¾µï¼ˆèªæ°£ã€æ¨™é»ï¼‰ã€‚
                3. å…§å¿ƒç¨ç™½çš„è™•ç†æ–¹å¼ã€‚
                4. å¦‚ä½•ç‡Ÿé€ æ‡¸å¿µã€‚
                ç¯„æœ¬å…§å®¹ï¼š\n{analysis_sample}
                """
                response = temp_model.generate_content(extract_prompt)
                st.session_state.style_guide = response.text
                st.success("é¢¨æ ¼æŒ‡å—æå–æˆåŠŸï¼")

    # é¡¯ç¤ºä¸¦å…è¨±æ‰‹å‹•ä¿®æ”¹é¢¨æ ¼æŒ‡å—
    st.session_state.style_guide = st.text_area(
        "ç›®å‰çš„é¢¨æ ¼æŒ‡å— (å¯æ‰‹å‹•èª¿æ•´):", 
        value=st.session_state.style_guide, 
        height=300
    )
    
    st.divider()
    max_ref_chars = st.slider("æ¯æ¬¡å°è©±åƒè€ƒå­—æ•¸ä¸Šé™", 10000, 200000, 50000)
    model_name = st.selectbox("é¸æ“‡æ¨¡å‹", ["gemini-2.5-pro", "gemini-2.5-flash"])
    
    if st.button("ğŸš¨ æ¸…é™¤å°è©±ç´€éŒ„"):
        st.session_state.messages = []
        st.session_state.chat_history = []
        st.rerun()

# --- è®€å–å·¥å…· ---
def get_safe_content(path, max_chars=50000):
    if not path or not os.path.exists(path):
        return ""
    files = [f for f in os.listdir(path) if f.endswith(('.txt', '.md'))]
    random.shuffle(files)
    current_chars = 0
    context = ""
    for file in files:
        if current_chars > max_chars: break
        try:
            with open(os.path.join(path, file), 'r', encoding='utf-8') as f:
                text = f.read()
                context += f"\n\n--- [ç¯„ä¾‹æª”: {file}] ---\n{text[:3000]}\n"
                current_chars += len(text)
        except: pass
    return context

# --- åˆå§‹åŒ– AI ---
if api_key:
    genai.configure(api_key=api_key)
    
    # é‡å°éŸ“åœ‹ç¶²æ–‡é¢¨æ ¼å„ªåŒ–çš„æŒ‡ä»¤
    system_prompt = (
        "ä½ æ˜¯ä¸€ä½ç²¾é€šéŸ“åœ‹ç¶²æ–‡ï¼ˆNaver Series/KakaoPageï¼‰çš„è³‡æ·±ç·¨è¼¯ã€‚"
        "ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šã€å¯«ä½œé¢¨æ ¼æŒ‡å—ã€åš´æ ¼å¯©æ ¸ä½¿ç”¨è€…çš„è‰ç¨¿ã€‚"
        "\n\nä½ çš„äº’å‹•è¦å‰‡ï¼š\n"
        "1. **åš´å¸«æ¨¡å¼**ï¼šçµ•å°ä¸è¦ä¸€å‘³ç¨±è®šã€‚å¦‚æœæ–‡å­—ä¸å¤ ã€éŸ“å‘³ã€ã€ç¯€å¥å¤ªæ…¢ã€å»¢è©±å¤ªå¤šï¼Œè«‹ç›´æ¥çµ¦äºˆç²¾æº–çš„æ‰¹è©•ã€‚\n"
        "2. **å°æ¨™æ”¹å¯«**ï¼šæ¯ä¸€æ¬¡å»ºè­°å¾Œï¼Œè«‹é™„ä¸Šä¸€æ®µç¬¦åˆé¢¨æ ¼æŒ‡å—çš„æ”¹å¯«ç¤ºç¯„ã€‚\n"
        "3. **ä¿æŒæŒ‡å—ç²¾ç¥**ï¼šå°è©±ä¸­è«‹æ™‚åˆ»åƒè€ƒå´é‚Šæ¬„å®šç¾©çš„å¯«ä½œç‰¹å¾µã€‚"
    )
    # å»ºç«‹æ¨¡å‹èˆ‡å°è©±ç‰©ä»¶ï¼ˆå°‡æ­·å²ç´€éŒ„å‚³å…¥ï¼‰
    model = genai.GenerativeModel(model_name=model_name, system_instruction=system_prompt)
    # é€™è£¡ä½¿ç”¨ st.session_state.chat_history ä¾†ä¿æŒé€£è²«æ€§
    chat_session = model.start_chat(history=st.session_state.chat_history)

# --- ä¸»ä»‹é¢ ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("è¼¸å…¥ä½ æƒ³è¨è«–çš„æ®µè½..."):
    if not api_key:
        st.error("è«‹å…ˆè¼¸å…¥ API Keyï¼")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # çµ„åˆ Context
        ref_sample = get_safe_content(ref_path, max_chars=max_ref_chars)
        my_draft = get_safe_content(novel_path, max_chars=30000)
        
        full_prompt = (
            f"ã€æ ¸å¿ƒé¢¨æ ¼æŒ‡å—ã€‘(å¿…é ˆåš´æ ¼éµå®ˆ):\n{st.session_state.style_guide}\n\n"
            f"ã€å…·é«”åƒè€ƒæ¨£æœ¬ã€‘:\n{ref_sample}\n\n"
            f"ã€æˆ‘çš„ä½œå“ä¸Šä¸‹æ–‡ã€‘:\n{my_draft}\n\n"
            f"ã€ç•¶å‰ä»»å‹™ã€‘: {prompt}"
        )

        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            try:
                response = chat_session.send_message(full_prompt, stream=True)
                for chunk in response:
                    full_response += chunk.text
                    response_placeholder.markdown(full_response + "â–Œ")
                response_placeholder.markdown(full_response)
                
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                st.session_state.chat_history = chat_session.history
            except Exception as e:
                st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
